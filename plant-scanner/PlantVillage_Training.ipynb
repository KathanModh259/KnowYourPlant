{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963d6ccd",
   "metadata": {},
   "source": [
    "# PlantVillage Training Notebook\n",
    "This notebook trains, validates, and tests a plant disease classifier using the PlantVillage dataset. It is designed to run on a GPU if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ee5db",
   "metadata": {},
   "source": [
    "## 1. Load and Verify Dataset Paths\n",
    "Set the root path to your PlantVillage dataset and verify the train/val/test folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8e7374-1c9b-4488-83ca-e2714921540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.25a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.7.0a0+ecf3bae40a.nv25.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.22.0a0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.1.0)\n",
      "Collecting kagglehub\n",
      "  Downloading kagglehub-1.0.0-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (70.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.5.0)\n",
      "Collecting kagglesdk<1.0,>=0.1.14 (from kagglehub)\n",
      "  Downloading kagglesdk-0.1.15-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kagglesdk<1.0,>=0.1.14->kagglehub) (4.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.1.31)\n",
      "Downloading kagglehub-1.0.0-py3-none-any.whl (70 kB)\n",
      "Downloading kagglesdk-0.1.15-py3-none-any.whl (160 kB)\n",
      "Installing collected packages: kagglesdk, kagglehub\n",
      "Successfully installed kagglehub-1.0.0 kagglesdk-0.1.15\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision numpy matplotlib scikit-learn pillow kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199cd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT CONTENTS:\n",
      "/root/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/versions/3/plantvillage dataset -> DIR\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Download directly from Kaggle (requires Kaggle API credentials)\n",
    "DATASET_ROOT = Path(kagglehub.dataset_download(\"abdallahalidev/plantvillage-dataset\"))\n",
    "\n",
    "# Optional: override with a local path instead of Kaggle download\n",
    "# DATASET_ROOT = Path(r\"D:\\KnowYourPlant\\datasets\\PlantVillage\")\n",
    "\n",
    "TRAIN_DIR = DATASET_ROOT / \"train\"\n",
    "VAL_DIR = DATASET_ROOT / \"val\"\n",
    "TEST_DIR = DATASET_ROOT / \"test\"\n",
    "\n",
    "RAW_DIR = None\n",
    "if not (TRAIN_DIR.exists() and VAL_DIR.exists() and TEST_DIR.exists()):\n",
    "    candidates = [\n",
    "        DATASET_ROOT / \"plantvillage dataset\" / \"color\",\n",
    "        DATASET_ROOT / \"PlantVillage\" / \"color\",\n",
    "        DATASET_ROOT / \"color\",\n",
    "        DATASET_ROOT,\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if candidate.exists():\n",
    "            RAW_DIR = candidate\n",
    "            break\n",
    "\n",
    "print(\"DATASET_ROOT:\", DATASET_ROOT)\n",
    "print(\"Train exists:\", TRAIN_DIR.exists())\n",
    "print(\"Val exists:\", VAL_DIR.exists())\n",
    "print(\"Test exists:\", TEST_DIR.exists())\n",
    "print(\"RAW_DIR:\", RAW_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec75a8-ee4c-45e7-a9f6-c34925ae7ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subdirectories: [PosixPath('/root/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/versions/3/plantvillage dataset')]\n",
      "Using: /root/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/versions/3/plantvillage dataset\n",
      "\n",
      "Classes inside:\n",
      "color\n",
      "grayscale\n",
      "segmented\n"
     ]
    }
   ],
   "source": [
    "if RAW_DIR is None:\n",
    "    print(\"RAW_DIR not found. Set DATASET_ROOT or create train/val/test splits.\")\n",
    "else:\n",
    "    class_dirs = [p for p in RAW_DIR.iterdir() if p.is_dir()]\n",
    "    print(\"RAW_DIR classes:\", len(class_dirs))\n",
    "    print(\"Sample classes:\", [p.name for p in class_dirs[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6d1dd-c8a4-4ad8-a6ab-a767b1dad14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: /root/.cache/kagglehub/datasets/abdallahalidev/plantvillage-dataset/versions/3/plantvillage dataset/color\n",
      "Exists: True\n",
      "Total classes: 38\n",
      "Sample classes: ['Soybean___healthy', 'Apple___Apple_scab', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Grape___Black_rot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus']\n"
     ]
    }
   ],
   "source": [
    "def summarize_split_dir(dir_path, name):\n",
    "    if not dir_path.exists():\n",
    "        print(f\"{name} missing: {dir_path}\")\n",
    "        return\n",
    "    class_dirs = [p for p in dir_path.iterdir() if p.is_dir()]\n",
    "    print(f\"{name} classes:\", len(class_dirs))\n",
    "    print(f\"{name} sample:\", [p.name for p in class_dirs[:5]])\n",
    "\n",
    "summarize_split_dir(TRAIN_DIR, \"train\")\n",
    "summarize_split_dir(VAL_DIR, \"val\")\n",
    "summarize_split_dir(TEST_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6dbde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete.\n"
     ]
    }
   ],
   "source": [
    "# Optional: split a single-folder PlantVillage dataset into train/val/test.\n",
    "# Use this if your dataset is organized as:\n",
    "# DATASET_ROOT/\n",
    "#   class_a/\n",
    "#   class_b/\n",
    "# and you do NOT already have train/val/test folders.\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "if RAW_DIR is None:\n",
    "    raise ValueError(\"RAW_DIR is not set. Update DATASET_ROOT or set RAW_DIR manually.\")\n",
    "\n",
    "split_ratio = (0.8, 0.1, 0.1)  # train, val, test\n",
    "seed = 42\n",
    "use_symlinks = False  # True can save space but may require admin on Windows\n",
    "\n",
    "def split_dataset(raw_dir, train_dir, val_dir, test_dir, ratios, seed, use_symlinks):\n",
    "    if train_dir.exists() and val_dir.exists() and test_dir.exists():\n",
    "        print(\"train/val/test already exist. Skipping split.\")\n",
    "        return\n",
    "\n",
    "    random.seed(seed)\n",
    "    class_dirs = [p for p in raw_dir.iterdir() if p.is_dir()]\n",
    "    if not class_dirs:\n",
    "        raise ValueError(f\"No class folders found in {raw_dir}\")\n",
    "\n",
    "    for class_dir in class_dirs:\n",
    "        images = [p for p in class_dir.iterdir() if p.is_file()]\n",
    "        random.shuffle(images)\n",
    "\n",
    "        n_total = len(images)\n",
    "        n_train = int(n_total * ratios[0])\n",
    "        n_val = int(n_total * ratios[1])\n",
    "        n_test = n_total - n_train - n_val\n",
    "\n",
    "        splits = {\n",
    "            train_dir / class_dir.name: images[:n_train],\n",
    "            val_dir / class_dir.name: images[n_train:n_train + n_val],\n",
    "            test_dir / class_dir.name: images[n_train + n_val:],\n",
    "        }\n",
    "\n",
    "        for out_dir, files in splits.items():\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            for src in files:\n",
    "                dst = out_dir / src.name\n",
    "                if use_symlinks:\n",
    "                    if dst.exists():\n",
    "                        continue\n",
    "                    os.symlink(src, dst)\n",
    "                else:\n",
    "                    if dst.exists():\n",
    "                        continue\n",
    "                    shutil.copy2(src, dst)\n",
    "\n",
    "    print(\"Split complete.\")\n",
    "\n",
    "split_dataset(RAW_DIR, TRAIN_DIR, VAL_DIR, TEST_DIR, split_ratio, seed, use_symlinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0119075",
   "metadata": {},
   "source": [
    "## 2. Install and Import Dependencies\n",
    "Install and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "126ed354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in a fresh environment, uncomment the line below\n",
    "# !pip install torch torchvision numpy matplotlib scikit-learn\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c2a7b",
   "metadata": {},
   "source": [
    "## 3. Configure GPU and Mixed Precision\n",
    "Detect CUDA, select device, and enable mixed precision if supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58ec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2837/1673106420.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "use_amp = use_cuda\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13439bd7",
   "metadata": {},
   "source": [
    "## 4. Define Data Transforms and Augmentations\n",
    "Create transforms for training and for validation/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eba588a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31ece6",
   "metadata": {},
   "source": [
    "## 5. Create Train/Validation/Test Dataloaders\n",
    "Use ImageFolder and DataLoader to build iterable loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b0d0a05",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Found no valid file for the classes plantvillage dataset. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      2\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(VAL_DIR, transform\u001b[38;5;241m=\u001b[39mval_test_transforms)\n\u001b[1;32m      6\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(TEST_DIR, transform\u001b[38;5;241m=\u001b[39mval_test_transforms)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py:150\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m    149\u001b[0m classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[0;32m--> 150\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py:203\u001b[0m, in \u001b[0;36mDatasetFolder.make_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_to_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py:104\u001b[0m, in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported extensions are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextensions\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(extensions,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instances\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes plantvillage dataset. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_workers = 2 if os.name == \"nt\" else 4\n",
    "\n",
    "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(VAL_DIR, transform=val_test_transforms)\n",
    "test_dataset = datasets.ImageFolder(TEST_DIR, transform=val_test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=use_cuda)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=use_cuda)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=use_cuda)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(\"Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c497959",
   "metadata": {},
   "source": [
    "## 6. Build the Model (Transfer Learning)\n",
    "Load a pretrained model and replace the classifier head for PlantVillage classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Replace the final layer\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad13a8c5",
   "metadata": {},
   "source": [
    "## 7. Set Loss, Optimizer, and Scheduler\n",
    "Configure criterion, optimizer, and learning-rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2748f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81772e41",
   "metadata": {},
   "source": [
    "## 8. Train the Model\n",
    "Implement a training loop with forward, backward, and metric tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c659cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\", enabled=scaler.is_enabled()):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe11db9",
   "metadata": {},
   "source": [
    "## 9. Validate Each Epoch\n",
    "Run validation after each epoch and track the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ab380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels).item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "num_epochs = 10\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_val_acc = 0.0\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train loss {train_loss:.4f} acc {train_acc:.4f} | Val loss {val_loss:.4f} acc {val_acc:.4f} | {elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452ce42",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set\n",
    "Compute final test metrics and optionally show a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test loss {test_loss:.4f} acc {test_acc:.4f}\")\n",
    "\n",
    "# Optional confusion matrix (requires scikit-learn)\n",
    "try:\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    disp.plot(ax=ax, xticks_rotation=\"vertical\")\n",
    "    plt.show()\n",
    "except Exception as exc:\n",
    "    print(\"Confusion matrix skipped:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d61bd4",
   "metadata": {},
   "source": [
    "## 11. Save and Load the Best Model\n",
    "Save best weights and demonstrate loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906293f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"plantvillage_resnet18_best.pth\"\n",
    "torch.save(best_model_wts, best_model_path)\n",
    "print(\"Saved:\", best_model_path)\n",
    "\n",
    "# Load later\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc105550",
   "metadata": {},
   "source": [
    "## 12. Inference on New Images\n",
    "Run prediction on a few sample images and map outputs to class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fb0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Update this to a few image paths\n",
    "sample_images = [\n",
    "    # r\"D:\\\\KnowYourPlant\\\\sample1.jpg\",\n",
    "    # r\"D:\\\\KnowYourPlant\\\\sample2.jpg\",\n",
    "]\n",
    "\n",
    "def predict_image(model, image_path, transform, class_names, device):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        conf, pred = torch.max(probs, 1)\n",
    "\n",
    "    return class_names[pred.item()], conf.item()\n",
    "\n",
    "for path in sample_images:\n",
    "    label, conf = predict_image(model, path, val_test_transforms, class_names, device)\n",
    "    print(f\"{path} -> {label} ({conf:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
